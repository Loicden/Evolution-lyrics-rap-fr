Résumés des réunions

12 FEVRIER : 

On a complété les champs lexicaux et créé un programme champs_lexicaux.py qui permet de trier les mots d'une année par champ lexicaux. On l'a appliqué à l'année 2020 ce qui a donné un fichier champs_2020.csv (pour chaque mot appartenant à un champ lexical déjà défini, donne le champ lexical auquel il appartient et sa proportion d'apparition dans l'année) et un fichier champs_restants 2020.csv qui donne tous les mots qui n'appartiennent pas encore à des champs lexicaux (triés par ordre de proportion d'apparition)

Infos données par le prof : 

le rendu (notebook observable) doit être constitué d'environ 4 graphes descriptifs de nos données + d'une visualisation principale (qui doit être mis en avant et accessible facilement, pas au 5eme click). La viz principale est la plus complexe, souvent non standard (par exemple avec une interaction particulière ou qui raconte une histoire). 
La documentation est aussi très importante (constitue plus ou moins la moitié du rendu). Mettre les liens des fichiers python, éventuellement des prototypes papier. Expliquer ce qu'on a enlevé, ce qu'on a sélectionné, les différents layouts explorés... 
Dans notre cas, il faudra détailler nos réflexions sur la normalisation pour pas introduire de biais, sur les difficultés liés au contexte d'un mot (noir = obscur ou black? sort = sortir ou le sort...) 


Idées brainstormées :

le site genius donne un dictionnaire d'argot qui pourrait être utile (soit pour nous donner des idées pour compléter les champs lexicaus, soit si on arrive à le scrapper pour étudier par exemple si certains rappers ont plus recours à l'argot que d'autres)

Faire attention à ne pas extraire des albums en double (rééditions)


https://genius.com/Genius-france-dictionnaire-des-mots-dargot-francais-annotated 

Pour pouvoir comparer les champs lexicaux, on va additionner les prportions des 5 mots les plus utilisés dans chaque champs. Ainsi, on enlève le biais du fait que certains champs lexicaux sont plus riches que d'autres (et donc la somme serait plus importante). 

ooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

5 FEVRIER:

On modifie le scrapper pour normaliser par musique plutôt que par album (ie pour chaque mot avoir le nombre de fois qu'il apparaît sur le nombre de mots dans la musique). 

(On pourra multiplier les proportions des mots des musiques d'un même album pour avoir les occurrences par album)

On ajoute des artistes/albums pour avoir plus de données (et on refait avec le nouveau scrapper ceux qui avaient déjà été faits).

Une fois qu'on aura plus d'artistes, on devrait pouvoir revoir/enrichir les champs lexicaux.

Nettoyer les résultats en enlevant les mots non signifiants ! 

Visualisations éventuelles:

Regarder si on trouve des algos tout faits d'analyse de sentiments. Permettrait de déterminer le style des artistes (éventuellement rapprocher les artistes avec le même style ou faire des nuages de mots avec une couleur pour le positif et une pour le négatif)

Regarder si y a des algos tout fait (par exemple t-SNE) qui permettrait de rapprocher les artistes (ou les musiques/albums) qui utilisent souvent les mêmes thèmes/champs lexicaux. 

-stacked bar chart ou radar chart pour visualiser les 5 premiers champs lexicaux en fonction des années et éventuellement en filtrant sur des artistes)

-treemap pour visualiser les champs lexicaux

-scatterplot avec la "richesse/diversité" de vocabulaire des artistes (peut-être pas pertinent, mais probablement facile à faire avec la fonction "set" de python)

ooooooooooooooooooooooooooooooooooooooooooooooooooooooo

MI-JANVIER :

-bdd sur une 20aine d'années, environ 10 artistes par année (on ajustera au besoin), 6 albums par atiste max

-2 reducers pour 2 fichiers csv : 1 par atiste et année et l'autre pour chaque année (javscript sert juste de filtre)

-viz : histogramme champs lexicaux par année / par artiste

-champs lexicaux faits à la main

-on stocke les paroles dans un fichier texte (a mettre sur github), qu'on reparcourera après pour appliquer le reducer. 

-on laisse les refrains apparaitre plusieurs fois

-on peut récupérer des dictionnaires de stop words tout faits (par exemple sur https://countwordsfree.com/stopwords/french)

-Loic met son code sur github qd le pb des pubs sera réglé
